---
title: Current state and prospects of R-packages for the design of experiments
author:
  - name: Emi Tanaka
    affiliation: Monash University
  - name: Dewi Amaliah 
    affiliation: Monash University
abstract: >
    Re-running an experiment is generally costly and in some cases impossible due to limited resources, so the design of an experiment plays a critical role in increasing the quality of experimental data.  In this article we describe the current state of the R-packages for the design of experiments through an exploratory data analysis of package downloads, package metadata, and the comparison of characteristics with other topics. We observe that experimental designs in practice appear to be sufficiently manufactured by a small number of packages and the development of experimental designs occur in silos. We discuss also the interface designs of widely utilised R packages in the field of experimental design and discuss its future prospects to advance the field in practice.
bibliography: paper.bib
knit: (function(inputFile, encoding) { rmarkdown::render(inputFile, encoding = encoding, output_file = file.path(dirname(inputFile), 'index.html')) })
keywords: ["experimental design", "CRAN task view", "interface design"]
output: 
  pagedown::html_paged:
    template: assets/html-template.html
    csl: assets/html-template.csl
    self_contained: false
    css: ["assets/html-template.css"]
---



```{r setup, cache = FALSE, include = FALSE}
library(targets)
library(glue)
library(ggwordcloud)
library(tidyverse)
library(patchwork)
library(tidygraph)
library(ggraph)
source("R/plot.R")
knitr::opts_chunk$set(echo = FALSE, 
                      cache = TRUE,
                      cache.path = "cache/",
                      fig.align = 'center', 
                      fig.pos = 'htbp', 
                      fig.path = "figures/",
                      fig.width = 6,
                      fig.height = 6.5,
                      message = FALSE,
                      warning = FALSE)


ctv <- function(view) {
  ifelse(knitr::is_html_output(), glue("[{view}](https://cran.r-project.org/web/views/{view}.html)"), glue("\\ctv{[view]}", .open = "[", .close = "]"))
}
CRANpkg <- function(pkg) {
  ifelse(knitr::is_html_output(), glue("[{pkg}](https://cran.r-project.org/web/packages/{pkg}/index.html)"), glue("\\CRANpkg{[pkg]}", .open = "[", .close = "]"))
}
ref <- function(x) {
  ifelse(knitr::is_html_output(), glue("\\@ref({x})"), glue("\\ref{[x]}", .open = "[", .close = "]"))
}
url <- function(x) {
  ifelse(knitr::is_html_output(), x, glue("\\url{[x]}", .open = "[", .close = "]"))
}


tar_load(npkgs)
tar_load(gini_yearly_doe)
tar_load(gini_yearly_ctv)
```



# Introduction 

The critical role of data collection is well captured in the expression "garbage in, garbage out" -- in other words, if the collected data is rubbish then no analysis, however complex it may be, can make something out of it. A carefully crafted data collection scheme is therefore critical to optimise the information from data. The field of experimental designs is specifically devoted to planning the collection of experimental data largely based on the founding principles by @Fisher1935-qc or an optimisation framework like those described in @Pukelsheim2006-sv. These experimental designs are often constructed with the aid of a statistical software, such as R [@R], so the usage of experimental design software can inform us about some aspects of experimental designs in practice. 

<!-- Unlike many other data collection schemes, experiments provides the greatest control and consequently, a greater opportunity to statistically optimise the information drawn from the experimental data. -->


Methods for data collection can be dichotomised by the type of data collected -- namely, experimental or observational -- or alternatively, categorised as experimental design (including quasi-experimental design) or survey design. This dichotomisation, to a great extent, is seen in the [Comprehensive R Archive Network (CRAN) task views](https://cran.r-project.org/web/views/) (a volunteer maintained list of R-packages by topic) where R-packages for experimental design are in `r ctv("ExperimentalDesign")` task view and R-packages for survey designs are in `r ctv("OfficialStatistics")` task view. The full list of available topics can be see in Table S1 in the Supplementary Materials. A subset of experimental designs are segregated into `r ctv("ClinicalTrials")` task view, where the focus is on clinical trials with primary interest in sample size calculations. This paper focuses on the packages in `r ctv("ExperimentalDesign")` task view, henceforth referred to as "DoE packages".

In `r ctv("ExperimentalDesign")` task view, there are `r npkgs` R packages for experimental design and analysis of data from experiments. The sheer quantity and variation of experimental designs in the R-packages are arguably unmatched with any other programming languages, e.g. in Python [@python], only a handful of packages that generate design of experiment exist (namely `pyDOE`, `pyDOE2`, `dexpy`, `experimenter` and `GPdoemd`) with limited type of designs. Thus, the study of DoE packages, based on quantitative and qualitative data, can give us an objective view to the state of the current experimental designs in practice. 


A utility of software can also be described by its design to facilitate clear expression and interpretation of the desired experimental design. Certain programming language design can hinder or discourage development of a reliable program [@Wasserman1975-xr].  The immense popularity of `tidyverse` [a collection of R-packages for various stages of data analysis that place enormous emphasis on the interface design by @Wickham2019-mj] is a testament to the impact an interface design can have in practice. The practice of experimental design could be advanced by adopting similar interface design principles across DoE packages. 



The paper is organised as follows. Section `r ref("data")` briefly describes the data source used for the analysis in Section `r ref("eda")`; Section `r ref("eda")` presents some insights into the state of the current DoE packages by exploratory data analysis of package download data, text descriptions and comparisons with other CRAN task views; Section `r ref("design")` discuss the interface designs of widely used DoE packages, and we conclude with a discussion in Section `r ref("discussion")` of future prospects in the software development of experimental designs. 

# Data {#data}

To study the DoE packages, we analyse data using three sources of data as described below. 

## RStudio CRAN download logs

The Comprehensive R Archive Network (CRAN) is a network of servers located across the world that store mirrored versions of R and R-packages. The most popular network is the RStudio mirror (the default server for those that use the RStudio IDE). The RStudio mirror is also the only server that provides a comprehensive daily download logs of R and R-packages since October 2012. The summary data can be easily accessed with the `cranlogs` package [@cranlogs]. This paper uses the data from the beginning of 2013 to end of 2021 (a total of 9 years) for the packages in the CRAN task views. 

## Package descriptions

All CRAN packages have a title, description, package connections (suggests, dependency and imports) and other meta-information in the DESCRIPTION file. We use the text data from the title and description (as accessed in `r as.Date(file.info("_targets/objects/ngram_download_data")$mtime)`) in Section `r ref("topics")`.

## CRAN task views

CRAN task views are volunteer maintained list of R-packages on CRAN relevant to the corresponding topic. There are a total `r n_distinct(gini_yearly_ctv$ctv)` CRAN task views. Table S1 in the Supplementary Materials list the full available topics from the `ctv` package [@ctv].
The list of packages in each CRAN task views (as of `r as.Date(file.info("_targets/objects/ctv_pkgs_data")$mtime)`) are used to contrast the characteristics of DoE packages in Section `r ref("silo")`. 


# Explorative data analysis {#eda}

In this section, we derive some conjecture based on the analysis of the data described in Section `r ref("data")`. All results presented are from the exploratory data analysis of observational data, consequently, all interpretations are somewhat speculative and may not be indicative of the true state of the field of experimental design. In particular, any analysis over time are confounded by the fact that the nature of users and package management have also changed over the years. It should be noted that some DoE packages may have been archived or removed from the task view over the years so any cross-sectional analysis presented may not reflect the set of all DoE packages at that particular time period (although we assume such incidences are low). 


A subset of DoE packages is not primarily about design of experiments but about the analysis of experimental data. A complete delineation of these packages is difficult as there is almost always at least one function that can aid decisions or constructions of experimental designs (and any categorisation is prone to our subjective bias) so we opted not to remove any DoE packages in the analysis.

## Small, but diverse, set of packages are sufficient for most experimental designs in practice {#popular}

There have been at least 50 DoE packages since 2013 but most of the downloads are concentrated in just a handful of packages. For example, Figure `r ref("fig:plot-lorenz")` shows a Lorenz curve [@Lorenz1905-tc] for the total package downloads in 2021 of `r tar_read(gini_yearly) %>% filter(year==2021) %>% pull(n)` DoE packages (first released prior to 2021); we can see from Figure `r ref("fig:plot-lorenz")` that bottom 90\% of DoE packages (in terms of total download count in 2021) only share about 30\% of total downloads across all DoE packages -- in another words, 70\% of the total downloads are due to 11 packages (10% of the DoE packages).

```{r plot-lorenz, fig.height = 4, fig.width = 4, fig.cap = "Lorenz curve of the total download count for DoE packages in 2021. The red line corresponds to the line of perfect equality. The green region shows the area under the Lorenz curve and the red region shows the area of gap in equality."}
tar_read(plot_lorenz2021) +
  plot_setup() +
  labs(y = "Cumulative share of downloads (%)",
       x = "Percentage of DoE packages") +
  theme(plot.margin = margin(r = 20, t = 20)) +
  scale_x_continuous(breaks = seq(0, 100, by = 10)) +
  scale_y_continuous(breaks = seq(0, 100, by = 10))
```

 If we consider package downloads as a measure of "wealth", then we can consider using the Gini index [@Gini1921-mf] as a measure of download inequality across packages. The ratio of the red region over the total colored regions in Figure `r ref("fig:plot-lorenz")` corresponds to the Gini index for 2021. A Gini index of 0\% indicates equality in downloads across packages while a value of 100\% indicates maximal inequality (all downloads are due to one package). In Figure `r ref("fig:download-share")`, we see that the distributions of the package downloads each year have a heavy right tail with the Gini index ranging from `r scales::percent(min(gini_yearly_doe$gini), 0.1)` to `r scales::percent(max(gini_yearly_doe$gini), 0.1)` across the years 2013 to 2021, indicating that there is a high level of inequality of package downloads, particularly with more pronounced inequality in the last 6 years.

```{r download-share, fig.width = 5.5, fig.height = 4, fig.cap = "The distribution of the number of downloads for each DoE package by year. Packages were removed in any year if it was released in that year or later so that each download count is for the full year. The label on the bottom of the plot shows the Gini index for downloads and the number of packages with full year of download count in the corresponding year. In the last 6 years, the Gini index is consistent above 60\\% each year indicating that most downloads are due to a relatively small number of packages. "}
tar_read(plot_download_dist) +
  labs(y = "Number of downloads")
```

The increase in the number of packages that are not highly downloaded may mean that **_there are more packages to construct niche experimental designs_**. Some examples of these packages include `qtlDesign`, `PwrGSD` and `Crossover` made for QTL experiments, group sequential designs and crossover trials, respectively. These packages would naturally have a smaller number of potential users. Counterfactual to this, the increase could be due to other external factors, such as an increase in the number of skilled developers (and thus more pacakge contributions), a change in CRAN policy or management to add packages (either to CRAN and/or task view), and/or that new packages are still yet to amass users. While there is an argument that low download counts are due to the low utility and/or quality of the packages, packages in CRAN task views are selected by expert maintainers, thus we can reasonably assume that any package listed in CRAN task view are of a decent utility and quality. 

If the downloads are reflective of the experimental designs used in practice, **_small set of packages appear to be sufficient for most to construct the full set of designs of experiments needed in practice_**. Packages of course evolve and the top downloaded packages have had regular updates that may have broadened its scope from its previous releases. 

While in absolute terms the Gini index is high for `r ctv("ExperimentalDesign")` task view (`r scales::percent(min(gini_yearly_doe$gini), 0.1)` to `r scales::percent(max(gini_yearly_doe$gini), 0.1)`), the inequality is not as severe as other CRAN task views as shown in Figure `r ref("fig:fig-gini-all-ctvs")`. We can see in Figure `r ref("fig:fig-gini-all-ctvs")` that the Gini index is generally increasing for DoE packages (as is generally the case for other CRAN task views as shown in Figure S1 in the Supplementary Materials) but most other CRAN task views have a Gini index of over 75%. This suggests that other CRAN task views may have dominant standards and in comparison to other topics, there are more **_diverse approaches to designing experiments_** and thus no single DoE package is dominant. This observation however doesn't take into account other approaches to generating experimental designs, like the proprietary software, CycDesignN [@cycdesignn] and SAS [@sas1985sas], which may be widely used.

```{r fig-gini-all-ctvs, fig.height = 4, fig.width = 4, fig.cap = "The points show the Gini index of the download counts by year for ExperimentalDesign task view with the color showing the number of packages. The grey line shows the distribution of the Gini index across years for all other CRAN task views. See Figure S1 in the Supplementary Material for the line graph for the Gini index across years by each CRAN task view."}
gini_yearly_ctv %>% 
  ungroup() %>%  
  arrange(ctv, year) %>% 
  mutate(ctv = fct_reorder(ctv, gini, last),
         year = factor(year),
         gini = 100 * gini) %>%  
  ggplot(aes(year, gini, group = ctv)) + 
  geom_line(color = "grey") +
  geom_line(data = ~filter(.x, ctv=="ExperimentalDesign")) + 
  geom_point(data = ~filter(.x, ctv=="ExperimentalDesign"),
             aes(color = n))  +
  plot_setup() +
  labs(x = "Year", y = "Gini index (%)", color = "# of packages",
       title = "ExperimentalDesign CRAN task view") +
  guides(color = guide_colorbar(direction = "horizontal")) +
  theme(legend.position = "bottom") +
  scale_colour_distiller(palette = "YlOrRd", direction = 1) 
```




## The field of experimental design is slow-changing  {#ranking}

We can see in Figure `r ref("fig:rank-over-time")` that most of the top 10 ranking packages have been in the top 10 for the last 9 years with `lhs` steadily climbing up the ranks in the last few years. It should be noted that the download of one package can prompt a download of another package; the most notable dependency network is `AlgDesign` and `agricolae`, where the former is an import for the latter. The full dependency network within the DoE packages is shown later in Figure \@ref(fig:plot-doe-network). 

```{r rank-over-time, fig.width = 5.5, fig.height = 4.5, fig.cap = "The plot shows the rank of top 10 downloaded packages by year. Packages that do not appear in top 10 for at least two periods are omitted from the plot. Most packages are consistently in the top 10 for the period shown."}
tar_read(plot_rank_dist) + labs(color = "Package")
```

Figure `r ref("fig:release-date-vs-download")` shows a moderate negative correlation between the first release date and the (log of) total download counts of DoE packages in any given year from 2013 to 2021. This suggest that in general, a package released earlier are more likely to be used today (possibly for legacy reasons or the general inertia to adopt new packages). We can also see in Figure `r ref("fig:release-date-vs-download")` the most downloaded packages were generally released in 2004 to 2010. 

```{r release-date-vs-download, fig.height = 5, fig.cap = "The above figure shows the total download (in log scale) of a package in the corresponding year against the first release date of the package. The blue line corresponds to the least squares fit of a simple linear regression model. The label in the right-hand upper corner shows the sample correlation coefficient between the first release date and the log (with base 10) of the total download count. The high-leverage point on the far left belongs to `conf.design`, authored by one of the earlier contributors to R."}
tar_read(cran_rank_yearly) %>%
  mutate(nupdates = map2_int(package, year, ~{
    tar_read(updates_all) %>% 
      filter(package == .x,
             lubridate::year(update) <= .y) %>% 
      nrow()
  })) %>% 
  ggplot(aes(first, total/1000)) +
  geom_point(aes(color = nupdates)) + 
  geom_label(data = ~ .x %>% 
              group_by(year) %>% 
              summarise(R = cor(as.numeric(first), log10(total/1000)) %>% 
                          scales::comma(0.01) %>% 
                          paste0("r = ", .),
                        y = quantile(total/1000, 0.99)),
            aes(label = R, y = y, x = as.Date("2014-11-01")),
            size = 3) +
  facet_wrap(~year, scale = "free_y") +
  geom_smooth(se = FALSE, 
              method = lm, 
              formula = y ~ x) +
  scale_y_log10() + 
  scale_color_continuous(trans = "log10") + 
  plot_setup() +
  labs(x = "First release date",
       y = "Total download (1000s)",
       color = "# of updates") +
  theme(legend.position = "bottom")
```

The consistency in the top 10 ranking packages (Figure `r ref("fig:rank-over-time")`) and the fact that most downloaded DoE packages are first released more than 10 years ago (Figure `r ref("fig:release-date-vs-download")`) indicates that either the existing packages filled the needs of mass in practice or no new packages were compelling for many to switch their practice. We do however also see that the the top downloaded packages generally have more updates (see Figure `r ref("fig:release-date-vs-download")`) so it is possible that the packages have improved or broadened the scope of its usage. 




## Optimal designs are of interest {#topics}

Figure `r ref("fig:wordcloud-over-time")` shows some common purposes of DoE packages, based on bigrams in the package title and description. We show only the bigrams as unigram was not insightful and there were not many trigrams common across packages. In counting the bigrams, we processed the text data as follows:

1. We standardised the words to lower case and removed pluralization. 
2. Multiple mentions of the same bigram within a package was counted as one (e.g. `AlgDesign` mentions "experimental design" four times in the title and the description but this is counted as one). 
3. Bigrams that consist the stop words (provided in `tidytext::stop_words` in addition to other words we deemed irrelevant, e.g. "provide", "e.g.", "calculate" and so on; full list is shown in the code provided in the link under Section `r ref("pkgs")`) are removed.

Not surprisingly, the bigram "experimental design" was the most common. More interestingly, "optimal design" and "sequential design" appeared across different packages (indicated by the size of the word in Figure `r ref("fig:wordcloud-over-time")`), and the bigrams "latin hypercube" and "computer experiment" are used across a few packages that are downloaded frequently (indicated by the color of the word in Figure `r ref("fig:wordcloud-over-time")`). Sequential design, Latin hypercube sampling and computer experiments (which generally include space filling designs like Latin hypercube sampling) are designs that generally operate by optimising a user-selected criterion and can be classified as optimal designs. 

```{r wordcloud-over-time, fig.width = 5.5, fig.height = 5, fig.cap = "The above figure shows the word cloud of bigrams from the title and descriptions of the DoE packages. The size shows how often the bigram appears across the DoE packages and the color are relative to the total download count in 2021 of the packages that contain the bigram."}
tar_read(plot_wordcloud2) + 
  scale_size_area(max_size = 5) +
  theme(legend.text = element_text(size = 10),
        legend.title = element_text(size = 12)) +
  labs(color = "Total downloads (%)")
```

Although there exists a separate `r ctv("ClinicalTrials")` task view, the DoE packages clearly include some packages that are of interest to clinical trials as shown by the size of the bigram "clinical trial" (and related bigrams like "dose finding" and "phase ii") in Figure `r ref("fig:wordcloud-over-time")`.

## Development of experimental designs occur in silos {#silo}

Figure `r ref("fig:ctv-summ-plot")` shows that the `r ctv("ExperimentalDesign")` have the lowest average number of contributors among all CRAN task views. In addition, we can also see in Figure `r ref("fig:ctv-summ-plot")`  that the `r ctv("ExperimentalDesign")` task view has one of the least intra-connectivity (the percentage of packages that make use of other packages within the same task view). The full connection of DoE packages with other DoE packages is shown in Figure `r ref("fig:plot-doe-network")`. These observations suggest that the field of experimental design is one of the least collaborative field and package development generally occur in silos. 



```{r ctv-summ-plot, fig.width = 5.5, fig.height = 6, fig.cap = "The above figure is a scatterplot of the intra-connectivity (the percentage of packages that depends, suggest or imports at least one other package within the same task view) and the average number of contributors for each CRAN task view. A low intra-connectivity suggests that development within the topic mostly occur in silos whilst high  intra-connectivity suggests that there are more interactions within the topic. The color shows the number of packages, the size of the point corresponds to the total number of contributors, and the text labels show the CRAN task view name.  The label of ExperimentalDesign task view is colored red. The task views in the bottom-left corner are topics that are more indicative of contributors working in silos. The actual numerical values are show in Table S1 in the Supplementary Material."}
tar_read(ctv_summary_table) %>% 
  ggplot(aes(avg_contributors, intra_perc)) +
  geom_point(aes(size = ncontributors,
                 color = n)) +
  ggnewscale::new_scale_color() + 
  ggrepel::geom_text_repel(aes(label = name,
                               color = name=="ExperimentalDesign"),
                           force = 150,
                           max.iter = 20000) +
  guides(color = "none") + 
  scale_color_manual(values = c("black", "#CC3311")) + 
  labs(x = "Average number of contributors",
       y = "Intra-connectivity (%)",
       size = "Total number of contributors",
       color_new = "Total number of packages") +
  plot_setup() +
  theme(legend.position = "bottom",
        legend.box = "vertical",
        legend.box.just = "left")
```

```{r plot-doe-network, fig.width = 6, fig.height = 6, fig.cap  = "Package connections (depends, suggests and imports) within the DoE packages. DoE packages that do not depend, suggest or import another DoE package is not shown."}
tar_read(plot_doe_network) +
  theme(legend.position = "bottom")
```

# Interface design  {#design}



In software design, there are two interface designs to consider: user interface (UI) and application programming interface (API). UI is concerned with the interaction of the software by the user, while the API is concerned with how different programs interact and is predominately of the interest to the developer. The UI design is an abstraction to specifying the desired experimental design and its choices enable how a user expresses the specification of the experimental design. 

In this section, we discuss these interface designs broadly, with examples illustrated from the top downloaded packages (shown in Figure `r ref("fig:rank-over-time")`). We exclude `ez` and `DiceKriging` from the discussion as the former  is predominately visualisation of experimental data and latter is about the analysis of computer experiments in addition to belonging to the same suite of packages as `DiceDesign` [@DiceDesign]. 



## The case of space-filling and optimal designs

Computer experiments, as discussed in Section `r ref("topics")`, which generally involve space filling designs, have been on the rise. The exemplar for this type of designs are `lhs` [@lhs] and `DiceDesign`. For the `lhs` package, functions like `randomLHS()`, `optimumLHS()`, and `maximinLHS()`, require users to specify the sample size ($n$) and the number of variables ($p$) then it generates a Latin hypercube sample, based on different optimisation schemes (in this case, random, S optimal and maxmin criteria, respectively; see package documentation for more details). Similarly for the `DiceDesign`, there is a comprehensive list of space-filling designs such as `dmaxDesign()`, `lhsDesign()`, `straussDesign()` and `wspDesign()` with input of $n$ and $p$ as before (among extra parameters for some) that implement algorithms that either maximise the entropy, produce a random Latin hypercube sample, based on Strauss process and WSP algorithm, respectively. These designs all output a $n \times p$ matrix with values between 0 and 1. 


The `AlgDesign` [@AlgDesign] package offers three primary functions for generating optimal designs: `optBlock()`, `optFederov()` and `optMonteCarlo()`. These functions in general require a formula in terms of the supplied terms with the choice of criterion (D, A and I for the latter two). The difference lies in the search algorithm for the optimal design.

## The case of factorial designs 

Factorial experiments offer a challenge in the construction and allocation of the treatment factors. This is reflected in a number of packages that specifically addresses this challenge. These include `DoE.base` [@DoE.base] for full factorial and (regular and irregular) orthogonal array designs via `fac.design()` and `oa.design()`, respectively, `FrF2` [@FrF2] for fractional 2-level factorial designs using `FrF2()`, `FrF2Large()` or `pb()` Plankett-Burnman designs [@Plackett1946-ly], `conf.design` [@conf.design] for symmetric confounded factorial designs via `conf.design()`, and `BHH2` [@BHH2] to generate full or fractional 2-level factorial design matrix via `ffDesMatrix()`. All these designs generally requires the user to input the number of factors and if the design is allowed to vary in the number of levels, the input of the number of levels for each factor. The construction of some of these designs are catalogue-based, e.g. `oa.design()` in `DoE.base`.


Response surface design, as illustrated by `rsm` [@rsm], is a factorial experiment but the focus of it is to analyse using response surface modelling. The two well known response surface designs are Box-Behnken design [@Box1960-wr] and
central-composite designs [@Box1951-ji] via `bbd()` and `ccd()` for central-composite designs providing formula.

## The case of sequential designs

Another specialised design is sequential design (also called adaptive sampling), which is best represented by `tgp` [@tgp] -- these require prior information, which are used to inform the next experimental design using `tgp.design()` and `dopt.gp()` which requires users to supply candidate samples to sub-sample from and a model or design from a prior design. Follow-up experiments, which can also be classified as a sequential design, is implemented by `BsMD` [@BsMD]. In `BsMD`, the follow-up design is determined by a model discriminant approach using `MD()`.

## The case of menu functions

The `agricolae` package [@agricolae] was the most downloaded DoE packages in 2013 to 2018 (Figure `r ref("fig:rank-over-time")`) and possibly prior to 2013 (data unavailable). This package is the prime example of constructing designs based on menu functions, e.g. `design.crd()`, `design.rcbd()` and `design.split()` construct a completely randomised design, randomised complete block design and split-plot design, respectively. Users typically supply the treatment labels (or number of treatments in the case of `design.split()`) and the number of replications as argument to these functions. 


# Discussion {#discussion}

Through the exploratory data analysis of three data sources (package download logs, package metadata and CRAN task views) outlined in Section `r ref("data")`, we have observed that the the total download of DoE packages are concentrated only on a handful of R-packages although these represent a diverse set in comparison with other CRAN task views. Furthermore, the data suggests that the field of experimental design is the least collaborative field; out of all CRAN task views, it has one of the least average number of authors and the lowest intra-connectivity (any dependency on other packages within its own task view). 
There are a number of limitations and short coming in our exploratory data analysis. First, CRAN task views are volunteer maintained so some experimental design packages may not be included in the DoE packages. Second, we only use the RStudio CRAN mirror download, which may bias our observations. Third, our analysis are limited to R-packages alone, although the sheer quantity and variation of experimental design is unmatched with any other programming languages. Finally, all our statements should be treated as speculative rather than conclusive; the data are all observational so no conclusive, generalisable statement are possible. Regardless, the data driven nature of our analysis give an objective insight into the field of experimental design. 

The interface design (discussed in Section `r ref("design")`) reveals that the most widely used DoE packages generally have functions that is 1) a menu format (i.e. the name of the function correspond to a particular recipe of the design or optimal search algorithm) and 2) context is often a second-thought (e.g. only the number of factors is needed and the package will assign pseudo factor names or an argument exists for optional input of character vector that corresponds to the factor names). 


`r if(knitr::is_latex_output()) '<!--'` 

# Supplemental materials 

The supplementary materials can be found [here](supp.pdf).

`r if(knitr::is_latex_output()) '-->'`

# Acknowledgement {#pkgs}

This paper uses `targets` [@targets] and `renv` [@renv] for reproducibility, `knitr` [@knitr] and `rmarkdown` [@rmarkdown] for creating reproducible documents, `ggplot2` [@ggplot2], `ggraph` [@ggraph], `ggwordcloud` [@ggwordcloud] and `colorspace` [@colorspace] for visualisation, `kableExtra` [@kableExtra] for customising the table in the Supplementary Material and `tidyverse` [@Wickham2019-mj], `tidytext` [@tidytext], `pluralize` [@pluralize], and `ineq` [@ineq] for data processing and manipulation, and `cranlogs` [@cranlogs] and `ctv` [@ctv] for extracting data. All code to reproduce this paper is found at `r url("https://github.com/emitanaka/paper-DoE-review")`.

# References